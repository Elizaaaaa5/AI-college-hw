{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8772 images belonging to 6 classes.\n",
      "Found 2238 images belonging to 6 classes.\n",
      "Epoch 1/16\n",
      "400/400 [==============================] - 207s 517ms/step - loss: 0.9251 - acc: 0.4658 - val_loss: 0.8781 - val_acc: 0.5250\n",
      "Epoch 2/16\n",
      "400/400 [==============================] - 182s 454ms/step - loss: 0.7974 - acc: 0.5752 - val_loss: 0.8969 - val_acc: 0.5615\n",
      "Epoch 3/16\n",
      "400/400 [==============================] - 176s 439ms/step - loss: 0.7133 - acc: 0.6623 - val_loss: 0.7207 - val_acc: 0.6961\n",
      "Epoch 4/16\n",
      "400/400 [==============================] - 173s 433ms/step - loss: 0.6410 - acc: 0.7074 - val_loss: 0.7063 - val_acc: 0.7047\n",
      "Epoch 5/16\n",
      "400/400 [==============================] - 179s 448ms/step - loss: 0.6089 - acc: 0.7223 - val_loss: 0.7844 - val_acc: 0.6786\n",
      "Epoch 6/16\n",
      "400/400 [==============================] - 175s 438ms/step - loss: 0.5807 - acc: 0.7357 - val_loss: 0.6512 - val_acc: 0.7290\n",
      "Epoch 7/16\n",
      "400/400 [==============================] - 174s 435ms/step - loss: 0.5389 - acc: 0.7560 - val_loss: 0.7233 - val_acc: 0.7171\n",
      "Epoch 8/16\n",
      "400/400 [==============================] - 176s 441ms/step - loss: 0.5099 - acc: 0.7717 - val_loss: 0.6793 - val_acc: 0.7336\n",
      "Epoch 9/16\n",
      "400/400 [==============================] - 180s 450ms/step - loss: 0.4889 - acc: 0.7822 - val_loss: 0.6637 - val_acc: 0.7413\n",
      "Epoch 10/16\n",
      "400/400 [==============================] - 179s 447ms/step - loss: 0.4542 - acc: 0.7977 - val_loss: 0.6702 - val_acc: 0.7394\n",
      "Epoch 11/16\n",
      "400/400 [==============================] - 176s 440ms/step - loss: 0.4346 - acc: 0.8050 - val_loss: 0.6558 - val_acc: 0.7460\n",
      "Epoch 12/16\n",
      "400/400 [==============================] - 194s 486ms/step - loss: 0.4068 - acc: 0.8180 - val_loss: 0.8031 - val_acc: 0.7041\n",
      "Epoch 13/16\n",
      "400/400 [==============================] - 180s 449ms/step - loss: nan - acc: 0.4931 - val_loss: nan - val_acc: 0.0119\n",
      "Epoch 14/16\n",
      "400/400 [==============================] - 175s 436ms/step - loss: nan - acc: 0.0098 - val_loss: nan - val_acc: 0.0128\n",
      "Epoch 15/16\n",
      "291/400 [====================>.........] - ETA: 38s - loss: nan - acc: 0.0093"
     ]
    }
   ],
   "source": [
    "# sources:\n",
    "# we used the Keras tutorial in order to understand how to create the neural network, as well as this site:\n",
    "# https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8\n",
    "# in order to export our .h5 (keras) files into .pb (tensorflow) format, we also used another site for reference:\n",
    "# https://stackoverflow.com/questions/45466020/how-to-export-keras-h5-to-tensorflow-pb\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "\n",
    "\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 6, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    directory=\"data/train\",\n",
    "    target_size=(64, 64),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    directory=r\"data/validation\",\n",
    "    target_size=(64, 64),\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 400,\n",
    "                         epochs = 16,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 200)\n",
    "\n",
    "classifier.save_weights('model.h5')\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in classifier.outputs])\n",
    "tf.train.write_graph(frozen_graph, \"\", \"model.pb\", as_text=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
